{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2964dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import typing\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c4129",
   "metadata": {},
   "source": [
    "We are looking for a way to tokenize strings of text. One possible way is to encode the text in unicode or UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a60b49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean)!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '안녕하세요 👋 (hello in Korean)!'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fedbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 41, 33]\n"
     ]
    }
   ],
   "source": [
    "print([ord(x) for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c3a0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (hello in Korean)!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042fa2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 41, 33]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c9074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 72, 197, 85, 177, 88, 213, 56, 193, 148, 198, 32, 0, 61, 216, 75, 220, 32, 0, 40, 0, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 0, 105, 0, 110, 0, 32, 0, 75, 0, 111, 0, 114, 0, 101, 0, 97, 0, 110, 0, 41, 0, 33, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-16')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f635062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 0, 0, 72, 197, 0, 0, 85, 177, 0, 0, 88, 213, 0, 0, 56, 193, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 75, 244, 1, 0, 32, 0, 0, 0, 40, 0, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 105, 0, 0, 0, 110, 0, 0, 0, 32, 0, 0, 0, 75, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 101, 0, 0, 0, 97, 0, 0, 0, 110, 0, 0, 0, 41, 0, 0, 0, 33, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd3964",
   "metadata": {},
   "source": [
    "Note how the UTF-16 and UTF-32 encodings above contain many 0s. This redundancy is wasteful, hence why we prefer UTF-8.\n",
    "\n",
    "However, a problem with UTF-8 is that characters are encoded into byte streams, meaning that we only have 256 possible tokens (a byte can take 256 possible values). This small vocabulary size leads to long, stretched-out vectors that embed the text, which eats up the model's context length quickly.\n",
    "\n",
    "We can solve this via the byte pair encoding algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ef94e",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189cca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Text: Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "Length of text: 533\n",
      "---\n",
      "Tokens: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length of tokens: 616\n"
     ]
    }
   ],
   "source": [
    "text = 'Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.'\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(f'Text: {text}')\n",
    "print(f'Length of text: {len(text)}')\n",
    "print('---')\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Length of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76406849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(code_points: typing.List[int]) -> typing.Dict[typing.Tuple[int, int], int]:\n",
    "    counts = dict()\n",
    "    for pair in zip(code_points, code_points[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed46435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((101, 32), 20), ((240, 159), 15), ((226, 128), 12), ((105, 110), 12), ((115, 32), 10), ((32, 97), 10), ((97, 110), 10), ((32, 116), 9), ((116, 104), 8), ((159, 133), 7), ((159, 135), 7), ((97, 114), 7), ((239, 189), 6), ((128, 140), 6), ((140, 240), 6), ((104, 101), 6), ((101, 114), 6), ((109, 101), 6), ((114, 32), 6), ((110, 100), 6), ((32, 105), 6), ((111, 114), 6), ((116, 32), 6), ((110, 103), 6), ((32, 115), 5), ((115, 116), 5), ((100, 101), 5), ((110, 32), 5), ((117, 115), 5), ((44, 32), 5), ((97, 109), 4), ((114, 105), 4), ((32, 102), 4), ((101, 97), 4), ((100, 32), 4), ((110, 116), 4), ((32, 111), 4), ((32, 119), 4), ((111, 117), 4), ((32, 85), 4), ((85, 110), 4), ((110, 105), 4), ((105, 99), 4), ((99, 111), 4), ((111, 100), 4), ((104, 97), 4), ((116, 101), 4), ((103, 32), 4), ((115, 44), 4), ((116, 105), 4), ((32, 240), 3), ((118, 101), 3), ((116, 114), 3), ((101, 115), 3), ((116, 111), 3), ((111, 32), 3), ((114, 116), 3), ((116, 115), 3), ((111, 102), 3), ((32, 112), 3), ((114, 115), 3), ((46, 32), 3), ((108, 108), 3), ((108, 32), 3), ((114, 101), 3), ((97, 116), 3), ((32, 109), 3), ((32, 98), 3), ((32, 100), 3), ((101, 110), 3), ((108, 101), 3), ((33, 32), 2), ((114, 121), 2), ((121, 32), 2), ((32, 110), 2), ((105, 107), 2), ((107, 101), 2), ((119, 101), 2), ((102, 32), 2), ((112, 114), 2), ((114, 111), 2), ((111, 103), 2), ((103, 114), 2), ((114, 97), 2), ((109, 109), 2), ((105, 100), 2), ((97, 108), 2), ((110, 111), 2), ((103, 104), 2), ((104, 116), 2), ((115, 117), 2), ((117, 112), 2), ((112, 112), 2), ((112, 111), 2), ((101, 226), 2), ((102, 116), 2), ((119, 104), 2), ((101, 118), 2), ((110, 115), 2), ((108, 105), 2), ((102, 111), 2), ((32, 114), 2), ((32, 99), 2), ((99, 97), 2), ((98, 101), 2), ((100, 105), 2), ((104, 111), 2), ((116, 97), 2), ((100, 97), 2), ((112, 108), 2), ((105, 116), 2), ((100, 111), 2), ((101, 112), 2), ((111, 110), 2), ((128, 153), 2), ((105, 111), 2), ((239, 188), 1), ((188, 181), 1), ((181, 239), 1), ((189, 142), 1), ((142, 239), 1), ((189, 137), 1), ((137, 239), 1), ((189, 131), 1), ((131, 239), 1), ((189, 143), 1), ((143, 239), 1), ((189, 132), 1), ((132, 239), 1), ((189, 133), 1), ((133, 33), 1), ((133, 164), 1), ((164, 240), 1), ((133, 157), 1), ((157, 240), 1), ((133, 152), 1), ((152, 240), 1), ((133, 146), 1), ((146, 240), 1), ((133, 158), 1), ((158, 240), 1), ((133, 147), 1), ((147, 240), 1), ((133, 148), 1), ((148, 226), 1), ((128, 189), 1), ((189, 32), 1), ((135, 186), 1), ((186, 226), 1), ((135, 179), 1), ((179, 226), 1), ((135, 174), 1), ((174, 226), 1), ((135, 168), 1), ((168, 226), 1), ((135, 180), 1), ((180, 226), 1), ((135, 169), 1), ((169, 226), 1), ((135, 170), 1), ((170, 33), 1), ((159, 152), 1), ((152, 132), 1), ((132, 32), 1), ((32, 84), 1), ((84, 104), 1), ((32, 118), 1), ((110, 97), 1), ((102, 101), 1), ((97, 119), 1), ((32, 104), 1), ((119, 111), 1), ((114, 108), 1), ((108, 100), 1), ((100, 119), 1), ((119, 105), 1), ((101, 46), 1), ((32, 87), 1), ((87, 101), 1), ((32, 107), 1), ((107, 110), 1), ((111, 119), 1), ((119, 32), 1), ((117, 103), 1), ((32, 226), 1), ((128, 156), 1), ((156, 115), 1), ((128, 157), 1), ((157, 32), 1), ((117, 114), 1), ((115, 111), 1), ((116, 119), 1), ((119, 97), 1), ((32, 40), 1), ((40, 119), 1), ((115, 226), 1), ((128, 148), 1), ((148, 108), 1), ((32, 117), 1), ((115, 105), 1), ((119, 99), 1), ((99, 104), 1), ((114, 95), 1), ((95, 116), 1), ((103, 115), 1), ((105, 103), 1), ((116, 63), 1), ((63, 41), 1), ((41, 46), 1), ((32, 66), 1), ((66, 117), 1), ((117, 116), 1), ((97, 98), 1), ((98, 115), 1), ((114, 117), 1), ((115, 101), 1), ((101, 44), 1), ((105, 118), 1), ((118, 105), 1), ((115, 97), 1), ((100, 45), 1), ((45, 112), 1), ((112, 97), 1), ((97, 103), 1), ((103, 101), 1), ((32, 83), 1), ((83, 116), 1), ((114, 100), 1), ((108, 117), 1), ((111, 122), 1), ((122, 101), 1), ((101, 109), 1), ((110, 110), 1), ((110, 101), 1), ((101, 120), 1), ((120, 101), 1), ((111, 116), 1), ((109, 111), 1), ((97, 32), 1), ((32, 108), 1), ((116, 116), 1), ((116, 108), 1), ((105, 109), 1), ((109, 105), 1), ((103, 46), 1), ((32, 73), 1), ((73, 32), 1), ((110, 226), 1), ((153, 116), 1), ((98, 108), 1), ((108, 97), 1), ((105, 108), 1), ((102, 105), 1), ((111, 108), 1), ((104, 105), 1), ((109, 121), 1), ((121, 115), 1), ((32, 101), 1), ((32, 51), 1), ((51, 48), 1), ((48, 32), 1), ((32, 121), 1), ((121, 101), 1), ((97, 102), 1), ((153, 115), 1), ((110, 99), 1), ((99, 101), 1), ((112, 116), 1), ((110, 46), 1)]\n"
     ]
    }
   ],
   "source": [
    "counts_dict = pair_counts(tokens)\n",
    "print(sorted(counts_dict.items(), key=lambda pair: pair[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c4fb7",
   "metadata": {},
   "source": [
    "Note from above, the token pair `(101, 32)` is the most commonly occurring pair of tokens in the text. Here's what that sequence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22578dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d541a4",
   "metadata": {},
   "source": [
    "Here is a Pythonic way to get the most commonly occurring pair from the dictionary of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c42aed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(counts_dict, key=counts_dict.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda7de6",
   "metadata": {},
   "source": [
    "Now we need a function that can merge a pair of tokens by a new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41b785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pairs(code_points: typing.List[int], pair_to_replace: typing.Tuple[int, int], replacement: int) -> typing.List[int]:\n",
    "    new_code_points = []\n",
    "    i = 0\n",
    "    while i < len(code_points):\n",
    "        if i < len(code_points) - 1 and code_points[i] == pair_to_replace[0] and code_points[i+1] == pair_to_replace[1]:\n",
    "            new_code_points.append(replacement)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_code_points.append(code_points[i])\n",
    "            i += 1\n",
    "    return new_code_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "171951ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 2, 3, 2, 4, 100, 9, 3, 2, 100]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_pairs([1, 2, 2, 3, 2, 4, 1, 2, 9, 3, 2, 1, 2], (1, 2), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee632736",
   "metadata": {},
   "source": [
    "We can replace the pair `(101, 32)` in our tokens with a new token: `256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd37523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n"
     ]
    }
   ],
   "source": [
    "old_length = len(tokens)\n",
    "tokens2 = replace_pairs(tokens, top_pair, 256)\n",
    "new_length = len(tokens2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7538faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length of tokens: 616, Length after replacing the top pair: 596, Difference: 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Original length of tokens: {old_length}, Length after replacing the top pair: {new_length}, Difference: {old_length - new_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa372df",
   "metadata": {},
   "source": [
    "Before applying the full byte pair encoding algorithm, we're going to get a larger piece of text to get better pair counting statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c46988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24636"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text is from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = open('unicode-article.txt', 'r', encoding='utf-8').read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31a59c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times to merge: 20\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = 256\n",
    "target_vocab_size = 276\n",
    "number_of_merges = target_vocab_size - original_vocab_size\n",
    "print(f'Number of times to merge: {number_of_merges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eca0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(\n",
    "    code_points: typing.List[int],\n",
    "    original_vocab_size: int,\n",
    "    number_of_merges: int\n",
    ") -> typing.Tuple[typing.List[int], typing.Dict[typing.Tuple[int, int], int]]:\n",
    "    # Make a copy of the code points list\n",
    "    replacements = OrderedDict()\n",
    "    for i in range(number_of_merges):\n",
    "        counts_dict = pair_counts(code_points)\n",
    "        top_pair = max(counts_dict, key=counts_dict.get)\n",
    "        new_index = original_vocab_size + i\n",
    "        print(f'Merging {top_pair} into new token {new_index}')\n",
    "        code_points = replace_pairs(code_points, top_pair, new_index)\n",
    "        replacements[top_pair] = new_index\n",
    "    return code_points, replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f406b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) into new token 256\n",
      "Merging (105, 110) into new token 257\n",
      "Merging (115, 32) into new token 258\n",
      "Merging (116, 104) into new token 259\n",
      "Merging (101, 114) into new token 260\n",
      "Merging (99, 111) into new token 261\n",
      "Merging (116, 32) into new token 262\n",
      "Merging (226, 128) into new token 263\n",
      "Merging (44, 32) into new token 264\n",
      "Merging (97, 110) into new token 265\n",
      "Merging (111, 114) into new token 266\n",
      "Merging (100, 32) into new token 267\n",
      "Merging (97, 114) into new token 268\n",
      "Merging (101, 110) into new token 269\n",
      "Merging (257, 103) into new token 270\n",
      "Merging (261, 100) into new token 271\n",
      "Merging (121, 32) into new token 272\n",
      "Merging (46, 32) into new token 273\n",
      "Merging (97, 108) into new token 274\n",
      "Merging (259, 256) into new token 275\n"
     ]
    }
   ],
   "source": [
    "bpe_tokens, replacements = bpe(tokens, original_vocab_size, number_of_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14753a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([((101, 32), 256),\n",
       "             ((105, 110), 257),\n",
       "             ((115, 32), 258),\n",
       "             ((116, 104), 259),\n",
       "             ((101, 114), 260),\n",
       "             ((99, 111), 261),\n",
       "             ((116, 32), 262),\n",
       "             ((226, 128), 263),\n",
       "             ((44, 32), 264),\n",
       "             ((97, 110), 265),\n",
       "             ((111, 114), 266),\n",
       "             ((100, 32), 267),\n",
       "             ((97, 114), 268),\n",
       "             ((101, 110), 269),\n",
       "             ((257, 103), 270),\n",
       "             ((261, 100), 271),\n",
       "             ((121, 32), 272),\n",
       "             ((46, 32), 273),\n",
       "             ((97, 108), 274),\n",
       "             ((259, 256), 275)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c2ec840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens length: 24636, Compressed tokens length: 19484, Compression ratio: 1.26x\n"
     ]
    }
   ],
   "source": [
    "print(f'Original tokens length: {len(tokens)}, Compressed tokens length: {len(bpe_tokens)}, Compression ratio: {len(tokens)/len(bpe_tokens):.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea7f2e",
   "metadata": {},
   "source": [
    "Now that we have trained the tokenizer using BPE, we can now perform encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4eb8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in replacements.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b6e046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: typing.List[int], vocab: typing.Dict[int, bytes]) -> str:\n",
    "    tokens = b''.join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a34d0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, replacements: typing.Dict[typing.Tuple[int, int], int]) -> typing.List[int]:\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        counts_dict = pair_counts(tokens)\n",
    "        pair = min(counts_dict, key=lambda p: replacements.get(p, float('inf')))\n",
    "        if pair not in replacements:\n",
    "            # Nothing else can be merged\n",
    "            break\n",
    "        idx = replacements[pair]\n",
    "        tokens = replace_pairs(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b338d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello world', replacements))\n",
    "print(decode(encode('hello world', replacements), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eae0427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = decode(encode(text, replacements), vocab)\n",
    "text == text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd23d6",
   "metadata": {},
   "source": [
    "## Splitting the Text into Mergeable Portions\n",
    "\n",
    "Sometimes we want to force the tokenizer to not merge certain tokens. We use the following regular expression to split the text into mergeable sub-texts. On each of these sub-texts, we apply the tokenizer individually and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "971493fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a8fd6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "560bd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"'re\", '    ', ' 123', 'world', \"'s\", '!!!?', '   ']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall(\"hello're     123world's!!!?   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469611c",
   "metadata": {},
   "source": [
    "Note that there are some issues with how this pre-processing step splits up the text. For example, uppercase suffixes and unicode apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa7aadd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', \"'s\", ' HOW', \"'\", 'S', ' how', '’', 's']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall(\"how's HOW'S how’s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e5769",
   "metadata": {},
   "source": [
    "We can also use the tiktoken library to use GPT's tokenizers. Notices how the `gpt2` tokenizer (used in GPT-2) does not merge spaces `220` is a space, whereas the `cl100k_base` tokenizer (used in GPT-4) merges multiple spaces into a single token `262`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053d7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 tokenizer tokenizing \"    hello world!!!\": [220, 220, 220, 23748, 995, 10185]\n",
      "cl100k_base tokenizer tokenizing \"    hello world!!!\": [262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "example_text = '    hello world!!!'\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f'gpt2 tokenizer tokenizing \"{example_text}\": {enc.encode(example_text)}')\n",
    "\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(f'cl100k_base tokenizer tokenizing \"{example_text}\": {enc.encode(example_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0c200",
   "metadata": {},
   "source": [
    "## minbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011aa524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
