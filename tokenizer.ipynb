{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2964dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c4129",
   "metadata": {},
   "source": [
    "We are looking for a way to tokenize strings of text. One possible way is to encode the text in unicode or UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a60b49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean)!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '안녕하세요 👋 (hello in Korean)!'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fedbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 41, 33]\n"
     ]
    }
   ],
   "source": [
    "print([ord(x) for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c3a0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xf0\\x9f\\x91\\x8b (hello in Korean)!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "042fa2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 41, 33]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c9074b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 72, 197, 85, 177, 88, 213, 56, 193, 148, 198, 32, 0, 61, 216, 75, 220, 32, 0, 40, 0, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 0, 105, 0, 110, 0, 32, 0, 75, 0, 111, 0, 114, 0, 101, 0, 97, 0, 110, 0, 41, 0, 33, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-16')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f635062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 0, 0, 72, 197, 0, 0, 85, 177, 0, 0, 88, 213, 0, 0, 56, 193, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 75, 244, 1, 0, 32, 0, 0, 0, 40, 0, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 105, 0, 0, 0, 110, 0, 0, 0, 32, 0, 0, 0, 75, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 101, 0, 0, 0, 97, 0, 0, 0, 110, 0, 0, 0, 41, 0, 0, 0, 33, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(s.encode('utf-32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd3964",
   "metadata": {},
   "source": [
    "Note how the UTF-16 and UTF-32 encodings above contain many 0s. This redundancy is wasteful, hence why we prefer UTF-8.\n",
    "\n",
    "However, a problem with UTF-8 is that characters are encoded into byte streams, meaning that we only have 256 possible tokens (a byte can take 256 possible values). This small vocabulary size leads to long, stretched-out vectors that embed the text, which eats up the model's context length quickly.\n",
    "\n",
    "We can solve this via the byte pair encoding algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56ef94e",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189cca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Text: Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "Length of text: 533\n",
      "---\n",
      "Tokens: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "Length of tokens: 616\n"
     ]
    }
   ],
   "source": [
    "text = 'Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.'\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(f'Text: {text}')\n",
    "print(f'Length of text: {len(text)}')\n",
    "print('---')\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Length of tokens: {len(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76406849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(code_points: typing.List[int]) -> typing.Dict[typing.Tuple[int, int], int]:\n",
    "    counts = dict()\n",
    "    for pair in zip(code_points, code_points[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed46435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((101, 32), 20), ((240, 159), 15), ((226, 128), 12), ((105, 110), 12), ((115, 32), 10), ((32, 97), 10), ((97, 110), 10), ((32, 116), 9), ((116, 104), 8), ((159, 133), 7), ((159, 135), 7), ((97, 114), 7), ((239, 189), 6), ((128, 140), 6), ((140, 240), 6), ((104, 101), 6), ((101, 114), 6), ((109, 101), 6), ((114, 32), 6), ((110, 100), 6), ((32, 105), 6), ((111, 114), 6), ((116, 32), 6), ((110, 103), 6), ((32, 115), 5), ((115, 116), 5), ((100, 101), 5), ((110, 32), 5), ((117, 115), 5), ((44, 32), 5), ((97, 109), 4), ((114, 105), 4), ((32, 102), 4), ((101, 97), 4), ((100, 32), 4), ((110, 116), 4), ((32, 111), 4), ((32, 119), 4), ((111, 117), 4), ((32, 85), 4), ((85, 110), 4), ((110, 105), 4), ((105, 99), 4), ((99, 111), 4), ((111, 100), 4), ((104, 97), 4), ((116, 101), 4), ((103, 32), 4), ((115, 44), 4), ((116, 105), 4), ((32, 240), 3), ((118, 101), 3), ((116, 114), 3), ((101, 115), 3), ((116, 111), 3), ((111, 32), 3), ((114, 116), 3), ((116, 115), 3), ((111, 102), 3), ((32, 112), 3), ((114, 115), 3), ((46, 32), 3), ((108, 108), 3), ((108, 32), 3), ((114, 101), 3), ((97, 116), 3), ((32, 109), 3), ((32, 98), 3), ((32, 100), 3), ((101, 110), 3), ((108, 101), 3), ((33, 32), 2), ((114, 121), 2), ((121, 32), 2), ((32, 110), 2), ((105, 107), 2), ((107, 101), 2), ((119, 101), 2), ((102, 32), 2), ((112, 114), 2), ((114, 111), 2), ((111, 103), 2), ((103, 114), 2), ((114, 97), 2), ((109, 109), 2), ((105, 100), 2), ((97, 108), 2), ((110, 111), 2), ((103, 104), 2), ((104, 116), 2), ((115, 117), 2), ((117, 112), 2), ((112, 112), 2), ((112, 111), 2), ((101, 226), 2), ((102, 116), 2), ((119, 104), 2), ((101, 118), 2), ((110, 115), 2), ((108, 105), 2), ((102, 111), 2), ((32, 114), 2), ((32, 99), 2), ((99, 97), 2), ((98, 101), 2), ((100, 105), 2), ((104, 111), 2), ((116, 97), 2), ((100, 97), 2), ((112, 108), 2), ((105, 116), 2), ((100, 111), 2), ((101, 112), 2), ((111, 110), 2), ((128, 153), 2), ((105, 111), 2), ((239, 188), 1), ((188, 181), 1), ((181, 239), 1), ((189, 142), 1), ((142, 239), 1), ((189, 137), 1), ((137, 239), 1), ((189, 131), 1), ((131, 239), 1), ((189, 143), 1), ((143, 239), 1), ((189, 132), 1), ((132, 239), 1), ((189, 133), 1), ((133, 33), 1), ((133, 164), 1), ((164, 240), 1), ((133, 157), 1), ((157, 240), 1), ((133, 152), 1), ((152, 240), 1), ((133, 146), 1), ((146, 240), 1), ((133, 158), 1), ((158, 240), 1), ((133, 147), 1), ((147, 240), 1), ((133, 148), 1), ((148, 226), 1), ((128, 189), 1), ((189, 32), 1), ((135, 186), 1), ((186, 226), 1), ((135, 179), 1), ((179, 226), 1), ((135, 174), 1), ((174, 226), 1), ((135, 168), 1), ((168, 226), 1), ((135, 180), 1), ((180, 226), 1), ((135, 169), 1), ((169, 226), 1), ((135, 170), 1), ((170, 33), 1), ((159, 152), 1), ((152, 132), 1), ((132, 32), 1), ((32, 84), 1), ((84, 104), 1), ((32, 118), 1), ((110, 97), 1), ((102, 101), 1), ((97, 119), 1), ((32, 104), 1), ((119, 111), 1), ((114, 108), 1), ((108, 100), 1), ((100, 119), 1), ((119, 105), 1), ((101, 46), 1), ((32, 87), 1), ((87, 101), 1), ((32, 107), 1), ((107, 110), 1), ((111, 119), 1), ((119, 32), 1), ((117, 103), 1), ((32, 226), 1), ((128, 156), 1), ((156, 115), 1), ((128, 157), 1), ((157, 32), 1), ((117, 114), 1), ((115, 111), 1), ((116, 119), 1), ((119, 97), 1), ((32, 40), 1), ((40, 119), 1), ((115, 226), 1), ((128, 148), 1), ((148, 108), 1), ((32, 117), 1), ((115, 105), 1), ((119, 99), 1), ((99, 104), 1), ((114, 95), 1), ((95, 116), 1), ((103, 115), 1), ((105, 103), 1), ((116, 63), 1), ((63, 41), 1), ((41, 46), 1), ((32, 66), 1), ((66, 117), 1), ((117, 116), 1), ((97, 98), 1), ((98, 115), 1), ((114, 117), 1), ((115, 101), 1), ((101, 44), 1), ((105, 118), 1), ((118, 105), 1), ((115, 97), 1), ((100, 45), 1), ((45, 112), 1), ((112, 97), 1), ((97, 103), 1), ((103, 101), 1), ((32, 83), 1), ((83, 116), 1), ((114, 100), 1), ((108, 117), 1), ((111, 122), 1), ((122, 101), 1), ((101, 109), 1), ((110, 110), 1), ((110, 101), 1), ((101, 120), 1), ((120, 101), 1), ((111, 116), 1), ((109, 111), 1), ((97, 32), 1), ((32, 108), 1), ((116, 116), 1), ((116, 108), 1), ((105, 109), 1), ((109, 105), 1), ((103, 46), 1), ((32, 73), 1), ((73, 32), 1), ((110, 226), 1), ((153, 116), 1), ((98, 108), 1), ((108, 97), 1), ((105, 108), 1), ((102, 105), 1), ((111, 108), 1), ((104, 105), 1), ((109, 121), 1), ((121, 115), 1), ((32, 101), 1), ((32, 51), 1), ((51, 48), 1), ((48, 32), 1), ((32, 121), 1), ((121, 101), 1), ((97, 102), 1), ((153, 115), 1), ((110, 99), 1), ((99, 101), 1), ((112, 116), 1), ((110, 46), 1)]\n"
     ]
    }
   ],
   "source": [
    "counts_dict = pair_counts(tokens)\n",
    "print(sorted(counts_dict.items(), key=lambda pair: pair[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c4fb7",
   "metadata": {},
   "source": [
    "Note from above, the token pair `(101, 32)` is the most commonly occurring pair of tokens in the text. Here's what that sequence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22578dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d541a4",
   "metadata": {},
   "source": [
    "Here is a Pythonic way to get the most commonly occurring pair from the dictionary of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c42aed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(counts_dict, key=counts_dict.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda7de6",
   "metadata": {},
   "source": [
    "Now we need a function that can merge a pair of tokens by a new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41b785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pairs(code_points: typing.List[int], pair_to_replace: typing.Tuple[int, int], replacement: int) -> typing.List[int]:\n",
    "    new_code_points = []\n",
    "    i = 0\n",
    "    while i < len(code_points):\n",
    "        if i < len(code_points) - 1 and code_points[i] == pair_to_replace[0] and code_points[i+1] == pair_to_replace[1]:\n",
    "            new_code_points.append(replacement)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_code_points.append(code_points[i])\n",
    "            i += 1\n",
    "    return new_code_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171951ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 2, 3, 2, 4, 100, 9, 3, 2, 100]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_pairs([1, 2, 2, 3, 2, 4, 1, 2, 9, 3, 2, 1, 2], (1, 2), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee632736",
   "metadata": {},
   "source": [
    "We can replace the pair `(101, 32)` in our tokens with a new token: `256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd37523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n"
     ]
    }
   ],
   "source": [
    "old_length = len(tokens)\n",
    "tokens2 = replace_pairs(tokens, top_pair, 256)\n",
    "new_length = len(tokens2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7538faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length of tokens: 616, Length after replacing the top pair: 596, Difference: 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Original length of tokens: {old_length}, Length after replacing the top pair: {new_length}, Difference: {old_length - new_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa372df",
   "metadata": {},
   "source": [
    "Before applying the full byte pair encoding algorithm, we're going to get a larger piece of text to get better pair counting statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c46988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24636"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text is from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = open('unicode-article.txt', 'r', encoding='utf-8').read()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a59c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times to merge: 20\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = 256\n",
    "target_vocab_size = 276\n",
    "number_of_merges = target_vocab_size - original_vocab_size\n",
    "print(f'Number of times to merge: {number_of_merges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca0bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(\n",
    "    code_points: typing.List[int],\n",
    "    original_vocab_size: int,\n",
    "    number_of_merges: int\n",
    ") -> typing.Tuple[typing.List[int], typing.Dict[typing.Tuple[int, int], int]]:\n",
    "    # Make a copy of the code points list\n",
    "    replacements = OrderedDict()\n",
    "    for i in range(number_of_merges):\n",
    "        counts_dict = pair_counts(code_points)\n",
    "        top_pair = max(counts_dict, key=counts_dict.get)\n",
    "        new_index = original_vocab_size + i\n",
    "        print(f'Merging {top_pair} into new token {new_index}')\n",
    "        code_points = replace_pairs(code_points, top_pair, new_index)\n",
    "        replacements[top_pair] = new_index\n",
    "    return code_points, replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f406b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) into new token 256\n",
      "Merging (105, 110) into new token 257\n",
      "Merging (115, 32) into new token 258\n",
      "Merging (116, 104) into new token 259\n",
      "Merging (101, 114) into new token 260\n",
      "Merging (99, 111) into new token 261\n",
      "Merging (116, 32) into new token 262\n",
      "Merging (226, 128) into new token 263\n",
      "Merging (44, 32) into new token 264\n",
      "Merging (97, 110) into new token 265\n",
      "Merging (111, 114) into new token 266\n",
      "Merging (100, 32) into new token 267\n",
      "Merging (97, 114) into new token 268\n",
      "Merging (101, 110) into new token 269\n",
      "Merging (257, 103) into new token 270\n",
      "Merging (261, 100) into new token 271\n",
      "Merging (121, 32) into new token 272\n",
      "Merging (46, 32) into new token 273\n",
      "Merging (97, 108) into new token 274\n",
      "Merging (259, 256) into new token 275\n"
     ]
    }
   ],
   "source": [
    "bpe_tokens, replacements = bpe(tokens, original_vocab_size, number_of_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14753a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([((101, 32), 256),\n",
       "             ((105, 110), 257),\n",
       "             ((115, 32), 258),\n",
       "             ((116, 104), 259),\n",
       "             ((101, 114), 260),\n",
       "             ((99, 111), 261),\n",
       "             ((116, 32), 262),\n",
       "             ((226, 128), 263),\n",
       "             ((44, 32), 264),\n",
       "             ((97, 110), 265),\n",
       "             ((111, 114), 266),\n",
       "             ((100, 32), 267),\n",
       "             ((97, 114), 268),\n",
       "             ((101, 110), 269),\n",
       "             ((257, 103), 270),\n",
       "             ((261, 100), 271),\n",
       "             ((121, 32), 272),\n",
       "             ((46, 32), 273),\n",
       "             ((97, 108), 274),\n",
       "             ((259, 256), 275)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c2ec840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens length: 24636, Compressed tokens length: 19484, Compression ratio: 1.26x\n"
     ]
    }
   ],
   "source": [
    "print(f'Original tokens length: {len(tokens)}, Compressed tokens length: {len(bpe_tokens)}, Compression ratio: {len(tokens)/len(bpe_tokens):.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea7f2e",
   "metadata": {},
   "source": [
    "Now that we have trained the tokenizer using BPE, we can now perform encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4eb8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in replacements.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b6e046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: typing.List[int], vocab: typing.Dict[int, bytes]) -> str:\n",
    "    tokens = b''.join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a34d0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, replacements: typing.Dict[typing.Tuple[int, int], int]) -> typing.List[int]:\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        counts_dict = pair_counts(tokens)\n",
    "        pair = min(counts_dict, key=lambda p: replacements.get(p, float('inf')))\n",
    "        if pair not in replacements:\n",
    "            # Nothing else can be merged\n",
    "            break\n",
    "        idx = replacements[pair]\n",
    "        tokens = replace_pairs(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b338d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode('hello world', replacements))\n",
    "print(decode(encode('hello world', replacements), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eae0427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = decode(encode(text, replacements), vocab)\n",
    "text == text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd23d6",
   "metadata": {},
   "source": [
    "## Splitting the Text into Mergeable Portions\n",
    "\n",
    "Sometimes we want to force the tokenizer to not merge certain tokens. We use the following regular expression to split the text into mergeable sub-texts. On each of these sub-texts, we apply the tokenizer individually and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971493fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a8fd6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall('hello world'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "560bd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', \"'re\", '    ', ' 123', 'world', \"'s\", '!!!?', '   ']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall(\"hello're     123world's!!!?   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469611c",
   "metadata": {},
   "source": [
    "Note that there are some issues with how this pre-processing step splits up the text. For example, uppercase suffixes and unicode apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa7aadd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', \"'s\", ' HOW', \"'\", 'S', ' how', '’', 's']\n"
     ]
    }
   ],
   "source": [
    "print(gpt2pat.findall(\"how's HOW'S how’s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e5769",
   "metadata": {},
   "source": [
    "We can also use the tiktoken library to use GPT's tokenizers. Notices how the `gpt2` tokenizer (used in GPT-2) does not merge spaces `220` is a space, whereas the `cl100k_base` tokenizer (used in GPT-4) merges multiple spaces into a single token `262`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "053d7da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 tokenizer tokenizing \"    hello world!!!\": [220, 220, 220, 23748, 995, 10185]\n",
      "cl100k_base tokenizer tokenizing \"    hello world!!!\": [262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "example_text = '    hello world!!!'\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(f'gpt2 tokenizer tokenizing \"{example_text}\": {enc.encode(example_text)}')\n",
    "\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(f'cl100k_base tokenizer tokenizing \"{example_text}\": {enc.encode(example_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4c334",
   "metadata": {},
   "source": [
    "## sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "349bef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61a2566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('toy.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16ca6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following options are from:\n",
    "# https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L#scrollTo=rSv1vfIVOhvr&line=2&uniqifier=1\n",
    "\n",
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "options = dict(\n",
    "    # input spec\n",
    "    input=\"toy.txt\",\n",
    "    input_format=\"text\",\n",
    "    # output spec\n",
    "    model_prefix=\"tok400\", # output filename prefix\n",
    "    # algorithm spec\n",
    "    # BPE alg\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=400,\n",
    "    # normalization\n",
    "    normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "    remove_extra_whitespaces=False,\n",
    "    input_sentence_size=200000000, # max number of training sentences\n",
    "    max_sentence_length=4192, # max number of bytes per sentence\n",
    "    seed_sentencepiece_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    # rare word treatment\n",
    "    character_coverage=0.99995,\n",
    "    byte_fallback=True,\n",
    "    # merge rules\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    split_by_whitespace=True,\n",
    "    split_by_number=True,\n",
    "    max_sentencepiece_length=16,\n",
    "    add_dummy_prefix=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    # special tokens\n",
    "    unk_id=0, # the UNK token MUST exist\n",
    "    bos_id=1, # the others are optional, set to -1 to turn off\n",
    "    eos_id=2,\n",
    "    pad_id=-1,\n",
    "    # systems\n",
    "    num_threads=os.cpu_count(), # use ~all system resources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b66c2dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=39\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 58\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20 all=283 active=244 piece=ed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=305 active=266 piece=.]\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=324 active=285 piece=ken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=80 all=334 active=295 piece=▁model\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=100 all=338 active=299 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "966785b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['▁s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['▁m', 271],\n",
       " ['▁u', 272],\n",
       " ['ing', 273],\n",
       " ['▁th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['▁S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['▁an', 290],\n",
       " ['▁pr', 291],\n",
       " ['▁to', 292],\n",
       " ['▁un', 293],\n",
       " ['▁the', 294],\n",
       " ['Piece', 295],\n",
       " ['▁Sentence', 296],\n",
       " ['▁SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['▁(', 309],\n",
       " ['▁[', 310],\n",
       " ['▁f', 311],\n",
       " ['▁n', 312],\n",
       " ['▁w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['▁Ne', 323],\n",
       " ['▁al', 324],\n",
       " ['▁de', 325],\n",
       " ['▁is', 326],\n",
       " ['▁ma', 327],\n",
       " ['▁mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['▁and', 332],\n",
       " ['▁lan', 333],\n",
       " ['▁pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['▁text', 337],\n",
       " ['▁model', 338],\n",
       " ['▁train', 339],\n",
       " ['kenizer', 340],\n",
       " ['▁system', 341],\n",
       " ['▁language', 342],\n",
       " ['▁training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc45fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 👋 (hello in Korean)! -> [362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151, 362, 243, 162, 148, 142, 309, 378, 361, 372, 358, 362, 262, 362, 399, 269, 361, 367, 363, 387, 36]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(s)\n",
    "print(f'{s} -> {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b439e9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>', '▁', '<0xF0>', '<0x9F>', '<0x91>', '<0x8B>', '▁(', 'h', 'e', 'l', 'lo', '▁', 'in', '▁', 'K', 'or', 'e', 'a', 'n', ')', '<0x21>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c39cb3",
   "metadata": {},
   "source": [
    "If we remove byte fallback, the unknown tokens do not get converted to bytes; they just stay unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df0ddf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: toy.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: toy.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=39\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 58\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20 all=283 active=244 piece=ed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=305 active=266 piece=.]\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=324 active=285 piece=ken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=80 all=334 active=295 piece=▁model\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=100 all=338 active=299 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=120 all=347 active=308 piece=▁v\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=140 all=354 active=315 piece=rat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=160 all=352 active=313 piece=lary\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=180 all=348 active=309 piece=igram\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=200 all=342 active=303 piece=▁where\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=220 all=327 active=288 piece=determined\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=240 all=308 active=269 piece=aw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=260 all=288 active=249 piece=ie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=280 all=268 active=229 piece=od\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=300 all=248 active=209 piece=tw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=0 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=320 all=228 active=189 piece=▁i\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=0 size=340 all=208 active=169 piece=ely\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "options_without_byte_fallback = dict(options)\n",
    "options_without_byte_fallback['byte_fallback'] = False\n",
    "spm.SentencePieceTrainer.train(**options_without_byte_fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cb60a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['en', 3],\n",
       " ['▁t', 4],\n",
       " ['ce', 5],\n",
       " ['in', 6],\n",
       " ['ra', 7],\n",
       " ['▁a', 8],\n",
       " ['de', 9],\n",
       " ['er', 10],\n",
       " ['▁s', 11],\n",
       " ['ent', 12],\n",
       " ['or', 13],\n",
       " ['pr', 14],\n",
       " ['▁m', 15],\n",
       " ['▁u', 16],\n",
       " ['ing', 17],\n",
       " ['▁th', 18],\n",
       " ['ence', 19],\n",
       " ['entence', 20],\n",
       " ['Pi', 21],\n",
       " ['ed', 22],\n",
       " ['em', 23],\n",
       " ['ex', 24],\n",
       " ['is', 25],\n",
       " ['iz', 26],\n",
       " ['la', 27],\n",
       " ['on', 28],\n",
       " ['st', 29],\n",
       " ['▁S', 30],\n",
       " ['Pie', 31],\n",
       " ['end', 32],\n",
       " ['ext', 33],\n",
       " ['▁an', 34],\n",
       " ['▁pr', 35],\n",
       " ['▁to', 36],\n",
       " ['▁un', 37],\n",
       " ['▁the', 38],\n",
       " ['Piece', 39],\n",
       " ['▁Sentence', 40],\n",
       " ['▁SentencePiece', 41],\n",
       " ['.]', 42],\n",
       " ['Ne', 43],\n",
       " ['ag', 44],\n",
       " ['do', 45],\n",
       " ['ec', 46],\n",
       " ['gu', 47],\n",
       " ['ic', 48],\n",
       " ['ir', 49],\n",
       " ['it', 50],\n",
       " ['ly', 51],\n",
       " ['to', 52],\n",
       " ['▁(', 53],\n",
       " ['▁[', 54],\n",
       " ['▁f', 55],\n",
       " ['▁n', 56],\n",
       " ['▁w', 57],\n",
       " ['.])', 58],\n",
       " ['age', 59],\n",
       " ['del', 60],\n",
       " ['ion', 61],\n",
       " ['ken', 62],\n",
       " ['lan', 63],\n",
       " ['ral', 64],\n",
       " ['wor', 65],\n",
       " ['yst', 66],\n",
       " ['▁Ne', 67],\n",
       " ['▁al', 68],\n",
       " ['▁de', 69],\n",
       " ['▁is', 70],\n",
       " ['▁ma', 71],\n",
       " ['▁mo', 72],\n",
       " ['izer', 73],\n",
       " ['rain', 74],\n",
       " ['ural', 75],\n",
       " ['▁and', 76],\n",
       " ['▁lan', 77],\n",
       " ['▁pre', 78],\n",
       " ['guage', 79],\n",
       " ['ystem', 80],\n",
       " ['▁text', 81],\n",
       " ['▁model', 82],\n",
       " ['▁train', 83],\n",
       " ['kenizer', 84],\n",
       " ['▁system', 85],\n",
       " ['▁language', 86],\n",
       " ['▁training', 87],\n",
       " ['.,', 88],\n",
       " ['BP', 89],\n",
       " ['Ku', 90],\n",
       " ['ab', 91],\n",
       " ['as', 92],\n",
       " ['at', 93],\n",
       " ['by', 94],\n",
       " ['co', 95],\n",
       " ['es', 96],\n",
       " ['et', 97],\n",
       " ['if', 98],\n",
       " ['ig', 99],\n",
       " ['im', 100],\n",
       " ['ke', 101],\n",
       " ['lo', 102],\n",
       " ['nr', 103],\n",
       " ['oc', 104],\n",
       " ['of', 105],\n",
       " ['om', 106],\n",
       " ['ot', 107],\n",
       " ['pa', 108],\n",
       " ['pl', 109],\n",
       " ['po', 110],\n",
       " ['pu', 111],\n",
       " ['re', 112],\n",
       " ['ry', 113],\n",
       " ['sp', 114],\n",
       " ['ss', 115],\n",
       " ['su', 116],\n",
       " ['te', 117],\n",
       " ['ub', 118],\n",
       " ['ws', 119],\n",
       " ['▁d', 120],\n",
       " ['▁g', 121],\n",
       " ['▁v', 122],\n",
       " ['BPE', 123],\n",
       " ['Sen', 124],\n",
       " ['abu', 125],\n",
       " ['bas', 126],\n",
       " ['cod', 127],\n",
       " ['det', 128],\n",
       " ['ect', 129],\n",
       " ['ene', 130],\n",
       " ['ens', 131],\n",
       " ['ere', 132],\n",
       " ['erm', 133],\n",
       " ['erv', 134],\n",
       " ['ich', 135],\n",
       " ['ior', 136],\n",
       " ['ith', 137],\n",
       " ['its', 138],\n",
       " ['ize', 139],\n",
       " ['oce', 140],\n",
       " ['ram', 141],\n",
       " ['rat', 142],\n",
       " ['raw', 143],\n",
       " ['rom', 144],\n",
       " ['sup', 145],\n",
       " ['▁by', 146],\n",
       " ['▁do', 147],\n",
       " ['▁et', 148],\n",
       " ['▁im', 149],\n",
       " ['▁ne', 150],\n",
       " ['▁of', 151],\n",
       " ['▁on', 152],\n",
       " ['▁pu', 153],\n",
       " ['▁us', 154],\n",
       " ['▁wh', 155],\n",
       " ['Kudo', 156],\n",
       " ['ecif', 157],\n",
       " ['ents', 158],\n",
       " ['ined', 159],\n",
       " ['inly', 160],\n",
       " ['ised', 161],\n",
       " ['lary', 162],\n",
       " ['lows', 163],\n",
       " ['pair', 164],\n",
       " ['pend', 165],\n",
       " ['plem', 166],\n",
       " ['post', 167],\n",
       " ['rely', 168],\n",
       " ['twor', 169],\n",
       " ['word', 170],\n",
       " ['▁dir', 171],\n",
       " ['▁end', 172],\n",
       " ['▁ext', 173],\n",
       " ['▁for', 174],\n",
       " ['▁not', 175],\n",
       " ['▁raw', 176],\n",
       " ['▁sub', 177],\n",
       " ['▁voc', 178],\n",
       " ['Sennr', 179],\n",
       " ['based', 180],\n",
       " ['encod', 181],\n",
       " ['igram', 182],\n",
       " ['ocess', 183],\n",
       " ['twork', 184],\n",
       " ['▁byte', 185],\n",
       " ['▁deto', 186],\n",
       " ['▁does', 187],\n",
       " ['▁from', 188],\n",
       " ['▁gene', 189],\n",
       " ['▁make', 190],\n",
       " ['▁size', 191],\n",
       " ['▁that', 192],\n",
       " ['▁with', 193],\n",
       " ['determ', 194],\n",
       " ['ecific', 195],\n",
       " ['ension', 196],\n",
       " ['postpr', 197],\n",
       " ['ration', 198],\n",
       " ['superv', 199],\n",
       " ['▁prior', 200],\n",
       " ['▁units', 201],\n",
       " ['▁where', 202],\n",
       " ['abulary', 203],\n",
       " ['▁Neural', 204],\n",
       " ['▁allows', 205],\n",
       " ['▁depend', 206],\n",
       " ['▁direct', 207],\n",
       " ['▁implem', 208],\n",
       " ['▁mainly', 209],\n",
       " ['▁neural', 210],\n",
       " ['▁purely', 211],\n",
       " ['Sennrich', 212],\n",
       " ['encoding', 213],\n",
       " ['entences', 214],\n",
       " ['ocessing', 215],\n",
       " ['specific', 216],\n",
       " ['▁Network', 217],\n",
       " ['▁subword', 218],\n",
       " ['▁systems', 219],\n",
       " ['▁unigram', 220],\n",
       " ['▁unsuperv', 221],\n",
       " ['determined', 222],\n",
       " ['▁extension', 223],\n",
       " ['▁sentences', 224],\n",
       " ['▁tokenizer', 225],\n",
       " ['▁generation', 226],\n",
       " ['▁implements', 227],\n",
       " ['▁vocabulary', 228],\n",
       " ['▁detokenizer', 229],\n",
       " ['▁unsupervised', 230],\n",
       " ['postprocessing', 231],\n",
       " ['▁predetermined', 232],\n",
       " ['PE', 233],\n",
       " ['Se', 234],\n",
       " ['])', 235],\n",
       " ['ai', 236],\n",
       " ['ak', 237],\n",
       " ['al', 238],\n",
       " ['am', 239],\n",
       " ['an', 240],\n",
       " ['ar', 241],\n",
       " ['aw', 242],\n",
       " ['ba', 243],\n",
       " ['bu', 244],\n",
       " ['bw', 245],\n",
       " ['ca', 246],\n",
       " ['ch', 247],\n",
       " ['ci', 248],\n",
       " ['ct', 249],\n",
       " ['di', 250],\n",
       " ['eP', 251],\n",
       " ['el', 252],\n",
       " ['ep', 253],\n",
       " ['eu', 254],\n",
       " ['fi', 255],\n",
       " ['fo', 256],\n",
       " ['fr', 257],\n",
       " ['ge', 258],\n",
       " ['gr', 259],\n",
       " ['ha', 260],\n",
       " ['he', 261],\n",
       " ['ie', 262],\n",
       " ['io', 263],\n",
       " ['le', 264],\n",
       " ['ll', 265],\n",
       " ['ma', 266],\n",
       " ['me', 267],\n",
       " ['mi', 268],\n",
       " ['mo', 269],\n",
       " ['mp', 270],\n",
       " ['ms', 271],\n",
       " ['nc', 272],\n",
       " ['nd', 273],\n",
       " ['ne', 274],\n",
       " ['ng', 275],\n",
       " ['ni', 276],\n",
       " ['nl', 277],\n",
       " ['nn', 278],\n",
       " ['no', 279],\n",
       " ['ns', 280],\n",
       " ['nt', 281],\n",
       " ['od', 282],\n",
       " ['oe', 283],\n",
       " ['ok', 284],\n",
       " ['os', 285],\n",
       " ['ow', 286],\n",
       " ['pe', 287],\n",
       " ['rd', 288],\n",
       " ['ri', 289],\n",
       " ['rk', 290],\n",
       " ['rm', 291],\n",
       " ['ro', 292],\n",
       " ['rv', 293],\n",
       " ['se', 294],\n",
       " ['si', 295],\n",
       " ['sy', 296],\n",
       " ['th', 297],\n",
       " ['ti', 298],\n",
       " ['tp', 299],\n",
       " ['tr', 300],\n",
       " ['ts', 301],\n",
       " ['tw', 302],\n",
       " ['ua', 303],\n",
       " ['ud', 304],\n",
       " ['ul', 305],\n",
       " ['un', 306],\n",
       " ['up', 307],\n",
       " ['ur', 308],\n",
       " ['us', 309],\n",
       " ['vi', 310],\n",
       " ['vo', 311],\n",
       " ['wh', 312],\n",
       " ['wi', 313],\n",
       " ['wo', 314],\n",
       " ['xt', 315],\n",
       " ['ys', 316],\n",
       " ['yt', 317],\n",
       " ['ze', 318],\n",
       " ['▁N', 319],\n",
       " ['▁b', 320],\n",
       " ['▁e', 321],\n",
       " ['▁i', 322],\n",
       " ['▁l', 323],\n",
       " ['▁o', 324],\n",
       " ['▁p', 325],\n",
       " ['▁r', 326],\n",
       " ['Net', 327],\n",
       " ['Neu', 328],\n",
       " ['ain', 329],\n",
       " ['air', 330],\n",
       " ['byt', 331],\n",
       " ['cab', 332],\n",
       " ['ceP', 333],\n",
       " ['ces', 334],\n",
       " ['dep', 335],\n",
       " ['din', 336],\n",
       " ['dir', 337],\n",
       " ['doe', 338],\n",
       " ['ece', 339],\n",
       " ['eci', 340],\n",
       " ['ede', 341],\n",
       " ['ely', 342],\n",
       " ['ems', 343],\n",
       " ['enc', 344],\n",
       " ['eni', 345],\n",
       " ['enn', 346],\n",
       " ['era', 347],\n",
       " ['fic', 348],\n",
       " ['for', 349],\n",
       " ['gen', 350],\n",
       " ['gra', 351],\n",
       " ['her', 352],\n",
       " ['imp', 353],\n",
       " ['ine', 354],\n",
       " ['ini', 355],\n",
       " ['inl', 356],\n",
       " ['lar', 357],\n",
       " ['lem', 358],\n",
       " ['low', 359],\n",
       " ['men', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2468664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 👋 (hello in Korean)! -> [362, 0, 362, 0, 53, 378, 252, 102, 362, 6, 362, 399, 13, 361, 240, 387, 0]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(s)\n",
    "print(f'{s} -> {ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c141391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<unk>', '▁', '<unk>', '▁(', 'h', 'el', 'lo', '▁', 'in', '▁', 'K', 'or', 'e', 'an', ')', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
